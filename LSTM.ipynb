{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, OrderedDict\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output, display_html\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torchtext\n",
    "from datasets import Dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from src.train import Train\n",
    "from src.train.ensemble import Model\n",
    "from src.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams[\"figure.figsize\"] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in Train, Validation, Test Dataset of Google Play Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2>Book Reviews Sentiment Analysis</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"<h2>Book Reviews Sentiment Analysis</h2>\"))\n",
    "loading_section       = [\"Prepare Model Data\"]\n",
    "sections              = [\"Train\", \"Evaluation\"]\n",
    "conclusion_section    = [\"Summary\"]\n",
    "\n",
    "train_sub_section   = [\"Ensemble Model\", \"LSTM\", \"BERT\"]\n",
    "me_sub_section      = [\"Model Performance\", \"Model Interpretability\"]\n",
    "\n",
    "accordions = OrderedDict()\n",
    "accordions[\"** Loading **\"] = widgets.Accordion(children=[widgets.Output() for section in loading_section])\n",
    "[accordions[\"** Loading **\"].set_title(i, section) for i, section in enumerate(loading_section)]\n",
    "\n",
    "for section in sections:\n",
    "    if section == \"Train\":\n",
    "        accordions[section] = widgets.Accordion(children=[widgets.Output() for sub_section in train_sub_section])\n",
    "        [accordions[section].set_title(i, sub_section) for i, sub_section in enumerate(train_sub_section)]\n",
    "    else:\n",
    "        accordions[section] = widgets.Accordion(children=[widgets.Output() for sub_section in me_sub_section])\n",
    "        [accordions[section].set_title(i, sub_section) for i, sub_section in enumerate(me_sub_section)]\n",
    "        \n",
    "accordions[\"** Conclusion **\"] = widgets.Accordion(children=[widgets.Output() for section in conclusion_section])\n",
    "[accordions[\"** Conclusion **\"].set_title(i, section) for i, section in enumerate(conclusion_section)]\n",
    "        \n",
    "widget_fields = widgets.Tab(children=[accordions[t] for t in accordions])\n",
    "[widget_fields.set_title(i, sub) for i, sub in enumerate(accordions.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd05fc3292343d3be095a6cbeda0942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Accordion(children=(Output(),), titles=('Prepare Model Data',)), Accordion(children=(Output(), Oâ€¦"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widget_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Train(target_var=\"sentiment\", predictives=\"reviews\", suffix=\"\")\n",
    "self = train\n",
    "\n",
    "# model = Model(m=CLASSFICATION_ALGORITHMS[\"LGBMC_TUNED\"], target_var=\"sentiment\", predictives=\"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "with accordions[\"** Loading **\"].children[0]:\n",
    "    clear_output()\n",
    "    display(Markdown(\"<h2> Initiating Modelling Data Preparation ... </h2>\"))\n",
    "    train.prepare_model_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(self.data[\"reviews_abt\"], test_size=self.MODELLING_CONFIG[\"TEST_SPLIT_RATIO\"], \n",
    "                                     shuffle=True, random_state=self.MODELLING_CONFIG[\"RANDOM_SEED\"])\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=self.MODELLING_CONFIG[\"VALIDATE_SPLIT_RATIO\"], \n",
    "                                    shuffle=True, random_state=self.MODELLING_CONFIG[\"RANDOM_SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121612, 8)\n",
      "(38005, 8)\n",
      "(30404, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def editDistance(str1, str2, m, n):\n",
    " \n",
    "    # If first string is empty, the only option is to\n",
    "    # insert all characters of second string into first\n",
    "    if m == 0:\n",
    "        return n\n",
    " \n",
    "    # If second string is empty, the only option is to\n",
    "    # remove all characters of first string\n",
    "    if n == 0:\n",
    "        return m\n",
    " \n",
    "    # If last characters of two strings are same, nothing\n",
    "    # much to do. Ignore last characters and get count for\n",
    "    # remaining strings.\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "        return editDistance(str1, str2, m-1, n-1)\n",
    " \n",
    "    # If last characters are not same, consider all three\n",
    "    # operations on last character of first string, recursively\n",
    "    # compute minimum cost for all three operations and take\n",
    "    # minimum of three values.\n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
    "                   editDistance(str1, str2, m-1, n),    # Remove\n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
    "                   )\n",
    " \n",
    " \n",
    "# Driver code\n",
    "str1 = \"deep\"\n",
    "str2 = \"creep\"\n",
    "print (editDistance(str1, str2, len(str1), len(str2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(example, tokenizer, max_length=256):\n",
    "    tokens = tokenizer(example[\"reviews\"])[:max_length]\n",
    "    length = len(tokens)\n",
    "    \n",
    "    return {\"tokens\": tokens, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  initiate English tokenizer from ...\n",
      "  creating Data Dictionary Object...\n",
      "    > train data\n",
      "    > test data\n",
      "    > validation data\n"
     ]
    }
   ],
   "source": [
    "self.logger.info(\"  initiate English tokenizer from ...\")\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "self.logger.info(\"  creating Data Dictionary Object...\")\n",
    "self.logger.info(\"    > train data\")\n",
    "train_dict = train_df[[\"reviews\", \"sentiment\"]].to_dict(orient=\"list\")\n",
    "train_data = Dataset.from_dict(train_dict)\n",
    "\n",
    "self.logger.info(\"    > test data\")\n",
    "test_dict = test_df[[\"reviews\", \"sentiment\"]].to_dict(orient=\"list\")\n",
    "test_data = Dataset.from_dict(test_dict)\n",
    "\n",
    "self.logger.info(\"    > validation data\")\n",
    "val_dict = val_df[[\"reviews\", \"sentiment\"]].to_dict(orient=\"list\")\n",
    "val_data = Dataset.from_dict(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  creating tokens & length of tokens info...\n",
      "    > train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/121612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > validation data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 128\n",
    "\n",
    "self.logger.info(\"  creating tokens & length of tokens info...\")\n",
    "self.logger.info(\"    > train data\")\n",
    "train_data = train_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})\n",
    "\n",
    "self.logger.info(\"    > test data\")\n",
    "test_data = test_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})\n",
    "\n",
    "self.logger.info(\"    > validation data\")\n",
    "val_data = val_data.map(tokenize_data, fn_kwargs={'tokenizer': tokenizer, 'max_length': max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_data[\"tokens\"],\n",
    "                                                  min_freq=min_freq,\n",
    "                                                  specials=special_tokens)\n",
    "\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_data(example, vocab):\n",
    "    ids = [vocab[token] for token in example[\"tokens\"]]\n",
    "    \n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  numericalize text data...\n",
      "    > train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/121612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > validation data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "self.logger.info(\"  numericalize text data...\")\n",
    "self.logger.info(\"    > train data\")\n",
    "train_data = train_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
    "self.logger.info(\"    > test data\")\n",
    "test_data = test_data.map(numericalize_data, fn_kwargs={'vocab': vocab})\n",
    "self.logger.info(\"    > validation data\")\n",
    "val_data = val_data.map(numericalize_data, fn_kwargs={'vocab': vocab})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transform data into Pytorch format...\n",
      "    > train data\n",
      "    > test data\n",
      "    > validation data\n"
     ]
    }
   ],
   "source": [
    "self.logger.info(\"  transform data into Pytorch format...\")\n",
    "self.logger.info(\"    > train data\")\n",
    "train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"sentiment\", \"length\"])\n",
    "self.logger.info(\"    > test data\")\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"sentiment\", \"length\"])\n",
    "self.logger.info(\"    > validation data\")\n",
    "val_data = val_data.with_format(type=\"torch\", columns=[\"ids\", \"sentiment\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.DataFrame.from_dict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121612, 5)\n",
      "(38005, 5)\n",
      "(30404, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='imdb_data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f, encoding='utf8') as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_dict[\"reviews\"], test_dict[\"reviews\"], \n",
    "                                                   train_dict[\"sentiment\"], test_dict[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    \n",
    "    for review in data:\n",
    "        for word in review:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = [k for k, v in sorted(word_count.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_dict = build_dict(train_dict[\"reviews\"])\n",
    "\n",
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of 51th review:  500\n",
      "All values in scope:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2211,  312, 3761, 1438, 2635, 2337,  490,  316, 2014,   38,    1,\n",
       "       1438,   52,    1, 1123,  685,  115, 2831,   95,  888,   39,    4,\n",
       "       3761,   90, 2211,  121,  804,  177, 1006, 1258,   27,  173,   84,\n",
       "        296,    1,   31,   79,  176,  182, 3916, 2584,  235,  623,  437,\n",
       "        867, 1448,    4,  285,  136,  579,  115,   27, 1566,  246,  563,\n",
       "       1306,   11,   31,    4,   76,  227,    1,  214, 1435, 2910,   72,\n",
       "        168,   20,    2,  139,   93,  326,   33,   71,    1,    5, 1269,\n",
       "       2472,  872,  237,   29,   11,  313,    1, 2814,  147,   84,    6,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Lenght of 51th review: \", len(train_X[50]))\n",
    "print(\"All values in scope: \", len([1 for num in train_X[50] if num >=0 and num <= 4998]) == len(train_X[50]))\n",
    "train_X[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>55</td>\n",
       "      <td>287</td>\n",
       "      <td>229</td>\n",
       "      <td>1267</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4095</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>3219</td>\n",
       "      <td>3007</td>\n",
       "      <td>954</td>\n",
       "      <td>2032</td>\n",
       "      <td>508</td>\n",
       "      <td>1246</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>514</td>\n",
       "      <td>348</td>\n",
       "      <td>233</td>\n",
       "      <td>184</td>\n",
       "      <td>16</td>\n",
       "      <td>186</td>\n",
       "      <td>580</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>1647</td>\n",
       "      <td>55</td>\n",
       "      <td>3731</td>\n",
       "      <td>243</td>\n",
       "      <td>819</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>12</td>\n",
       "      <td>91</td>\n",
       "      <td>536</td>\n",
       "      <td>643</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>243</td>\n",
       "      <td>694</td>\n",
       "      <td>411</td>\n",
       "      <td>335</td>\n",
       "      <td>2744</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "      <td>719</td>\n",
       "      <td>341</td>\n",
       "      <td>2224</td>\n",
       "      <td>2101</td>\n",
       "      <td>531</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>224</td>\n",
       "      <td>403</td>\n",
       "      <td>4365</td>\n",
       "      <td>11</td>\n",
       "      <td>1886</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>849</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>506</td>\n",
       "      <td>197</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>659</td>\n",
       "      <td>20</td>\n",
       "      <td>3631</td>\n",
       "      <td>241</td>\n",
       "      <td>2239</td>\n",
       "      <td>1</td>\n",
       "      <td>666</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    0    0    1     2     3     4     5     6     7    ...  490  491   \n",
       "0        0   79   55  287   229  1267     1     1  4095     1  ...    0    0  \\\n",
       "1        1  126    1   76  3219  3007   954  2032   508  1246  ...    0    0   \n",
       "2        0  225    2  514   348   233   184    16   186   580  ...    0    0   \n",
       "3        0  135    3   87     1  1647    55  3731   243   819  ...    0    0   \n",
       "4        0  189   12   91   536   643    10     1     3   150  ...    0    0   \n",
       "...    ...  ...  ...  ...   ...   ...   ...   ...   ...   ...  ...  ...  ...   \n",
       "24995    0   82  243  694   411   335  2744    97    95    63  ...    0    0   \n",
       "24996    1   44   37  719   341  2224  2101   531    30    17  ...    0    0   \n",
       "24997    0  134  224  403  4365    11  1886     2    49    37  ...    0    0   \n",
       "24998    1  166   29   20   849    59    12    28   506   197  ...    0    0   \n",
       "24999    1  354  659   20  3631   241  2239     1   666     4  ...    0    0   \n",
       "\n",
       "       492  493  494  495  496  497  498  499  \n",
       "0        0    0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "24995    0    0    0    0    0    0    0    0  \n",
       "24996    0    0    0    0    0    0    0    0  \n",
       "24997    0    0    0    0    0    0    0    0  \n",
       "24998    0    0    0    0    0    0    0    0  \n",
       "24999    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[25000 rows x 502 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_X, transformed_train_train_X_len = convert_and_pad_data(word_dict, train_dict[\"reviews\"])\n",
    "transformed_test_X, transformed_test_X_len = convert_and_pad_data(word_dict, test_dict[\"reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 1, 0, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of 51th review:  500\n",
      "All values in scope:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4,  3, 10, 10,  7, 21, 12, 18,  2,  7, 16, 22, 10,  3,  8,  8,  2,\n",
       "       21,  6,  6, 26,  2, 21, 15,  4,  8, 14, 11,  3, 10,  2,  5, 22, 22,\n",
       "        3,  5, 10,  2, 16,  3, 10,  3, 12, 18,  2,  4, 10, 18,  2, 22, 12,\n",
       "        5,  4, 11,  2, 14,  5,  8, 11,  2, 14,  6, 20,  2, 22, 10,  3,  4,\n",
       "        4, 18,  2, 14,  6, 12, 12,  3, 14,  4,  3, 13,  2, 22,  6,  3, 16,\n",
       "        2, 22, 12,  5,  4, 11,  2, 22, 12,  5,  4, 11,  2, 16, 18,  4, 11,\n",
       "        2, 19, 15,  5, 10,  5,  9,  4,  3,  3,  2,  8,  5, 12,  3,  2, 13,\n",
       "        7,  8, 12,  7, 26,  3,  2, 17,  5, 14,  4,  2,  4,  3,  9, 13,  2,\n",
       "       15,  8,  3,  2, 22, 10,  7, 16,  5, 10, 18,  2,  8,  6, 15, 10, 14,\n",
       "        3,  2, 13,  3,  8, 22,  7,  4,  3,  2, 17,  5, 14,  4,  2, 16,  5,\n",
       "       43,  6, 10,  2, 22, 12,  5, 18,  3, 10,  2, 12,  7, 17,  3,  2,  4,\n",
       "        3, 13,  2, 11, 15, 19, 11,  3,  8,  2,  5, 15, 10,  3, 12,  7,  5,\n",
       "        2, 22, 12,  5,  4, 11,  2,  5, 12,  7, 24,  3,  2,  5, 22, 22,  3,\n",
       "        5, 10,  2,  8,  3, 14,  6,  9, 13,  5, 10, 18,  2,  4,  3, 10,  4,\n",
       "        7,  5, 10, 18,  2,  8,  6, 15, 10, 14,  3,  2, 14,  6,  9,  8,  4,\n",
       "       10,  5,  7,  9,  4,  2, 22, 12,  5,  4, 11,  2,  3,  8,  4,  5,  4,\n",
       "        3,  2,  3,  8,  8,  3,  9,  4,  7,  5, 12, 12, 18,  2,  4,  3, 13,\n",
       "        2, 11, 15, 19, 11,  3,  8,  2,  8,  7,  8,  4,  3, 10,  2,  6, 12,\n",
       "       20, 18,  9,  2,  3,  9, 43,  6, 18,  2, 21,  6,  6, 26,  2, 10,  5,\n",
       "        9, 26,  2,  6, 22,  7,  9,  7,  6,  9,  2, 14,  5, 10, 12,  2, 10,\n",
       "        6, 12, 12, 18,  8,  6,  9,  2,  5, 16,  3, 10,  7, 14,  5,  2,  7,\n",
       "        8,  7,  2, 16,  6,  9,  3, 18,  2, 19, 10,  5, 21,  2,  3, 13,  7,\n",
       "        4,  6, 10,  2,  5, 19,  3,  9,  4,  2, 22, 15, 21, 12,  7,  8, 11,\n",
       "        3, 10,  2,  5, 15,  4, 11,  6, 10,  2,  3, 16, 21,  5, 10,  5,  8,\n",
       "        8,  2, 22,  7,  3, 14,  3,  2,  4, 10,  7, 22,  3,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Lenght of 51th review: \", len(transformed_train_X[50]))\n",
    "print(\"All values in scope: \", len([1 for num in transformed_train_X[50] if num >=0 and num <= 4998]) == len(transformed_train_X[50]))\n",
    "transformed_train_X[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.word_dict = None\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            ps = model(batch_X)\n",
    "            loss = loss_fn(ps, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "    \n",
    "    model_id = str(datetime.now()).split(\" \")[0].replace(\"-\", \"_\") + \\\n",
    "            str(datetime.now()).split(\" \")[1].split(\".\")[0].replace(\":\", \"_\")\n",
    "    model_path = os.path.join(self.FILES[\"MODEL_DATA_DIR\"], \"lstm_model_{}.pth\".format(model_id))\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_sample = pd.concat([pd.DataFrame(train_dict[\"sentiment\"]), \n",
    "                                    pd.DataFrame(transformed_train_train_X_len), \n",
    "                                    pd.DataFrame(transformed_train_X)], axis=1)\n",
    "transform_train_sample.columns = [str(i) for i in range(len(transform_train_sample.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6997045278549194\n",
      "Epoch: 2, BCELoss: 0.6906813263893128\n",
      "Epoch: 3, BCELoss: 0.6836840033531189\n",
      "Epoch: 4, BCELoss: 0.6765644550323486\n",
      "Epoch: 5, BCELoss: 0.6684593081474304\n",
      "Epoch: 6, BCELoss: 0.658358097076416\n",
      "Epoch: 7, BCELoss: 0.6447415709495544\n",
      "Epoch: 8, BCELoss: 0.6248663425445556\n",
      "Epoch: 9, BCELoss: 0.5931399941444397\n",
      "Epoch: 10, BCELoss: 0.5539361834526062\n",
      "Epoch: 11, BCELoss: 0.4990801870822906\n",
      "Epoch: 12, BCELoss: 0.44975553154945375\n",
      "Epoch: 13, BCELoss: 0.40791605710983275\n",
      "Epoch: 14, BCELoss: 0.3401987373828888\n",
      "Epoch: 15, BCELoss: 0.3005742937326431\n",
      "Epoch: 16, BCELoss: 0.24265267252922057\n",
      "Epoch: 17, BCELoss: 0.281191948056221\n",
      "Epoch: 18, BCELoss: 0.2682305365800858\n",
      "Epoch: 19, BCELoss: 0.1890871435403824\n",
      "Epoch: 20, BCELoss: 0.22829505801200867\n",
      "Epoch: 21, BCELoss: 0.17231179475784303\n",
      "Epoch: 22, BCELoss: 0.13192037045955657\n",
      "Epoch: 23, BCELoss: 0.13077259063720703\n",
      "Epoch: 24, BCELoss: 0.09353011697530747\n",
      "Epoch: 25, BCELoss: 0.06921389326453209\n",
      "Epoch: 26, BCELoss: 0.06605177223682404\n",
      "Epoch: 27, BCELoss: 0.04624384753406048\n",
      "Epoch: 28, BCELoss: 0.02999134175479412\n",
      "Epoch: 29, BCELoss: 0.021964151039719583\n",
      "Epoch: 30, BCELoss: 0.017538803815841674\n",
      "Epoch: 31, BCELoss: 0.012414730712771415\n",
      "Epoch: 32, BCELoss: 0.009578244294971228\n",
      "Epoch: 33, BCELoss: 0.007176239881664515\n",
      "Epoch: 34, BCELoss: 0.006231950223445892\n",
      "Epoch: 35, BCELoss: 0.005132349347695709\n",
      "Epoch: 36, BCELoss: 0.004375000949949026\n",
      "Epoch: 37, BCELoss: 0.003772731125354767\n",
      "Epoch: 38, BCELoss: 0.0033385366201400756\n",
      "Epoch: 39, BCELoss: 0.0030353303533047437\n",
      "Epoch: 40, BCELoss: 0.0027711969101801514\n",
      "Epoch: 41, BCELoss: 0.0025391281116753815\n",
      "Epoch: 42, BCELoss: 0.0023458136711269617\n",
      "Epoch: 43, BCELoss: 0.0021836627274751663\n",
      "Epoch: 44, BCELoss: 0.002042062790133059\n",
      "Epoch: 45, BCELoss: 0.0019183171447366477\n",
      "Epoch: 46, BCELoss: 0.001807246496900916\n",
      "Epoch: 47, BCELoss: 0.0017073319992050528\n",
      "Epoch: 48, BCELoss: 0.0016170194838196039\n",
      "Epoch: 49, BCELoss: 0.0015339661855250596\n",
      "Epoch: 50, BCELoss: 0.0014579801587387919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(5000, 32, padding_idx=0)\n",
       "  (lstm): LSTM(32, 100)\n",
       "  (dense): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 50, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "hidden_dim = 100\n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the parameters used to construct the model\n",
    "model_id = str(datetime.now()).split(\" \")[0].replace(\"-\", \"_\") + \\\n",
    "            str(datetime.now()).split(\" \")[1].split(\".\")[0].replace(\":\", \"_\")\n",
    "model_info_path = os.path.join(self.FILES[\"MODEL_DATA_DIR\"], \"model_info_{}.pth\".format(model_id))\n",
    "with open(model_info_path, \"wb\") as f:\n",
    "    model_info = {\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"vocab_size\": vocab_size,\n",
    "    }\n",
    "    torch.save(model_info, f)\n",
    "\n",
    "# Save the word_dict\n",
    "word_dict_path = os.path.join(self.FILES[\"MODEL_DATA_DIR\"], \"word_dict_{}.pkl\".format(model_id))\n",
    "with open(word_dict_path, \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)\n",
    "\n",
    "# Save the model parameters\n",
    "model_path = os.path.join(self.FILES[\"MODEL_DATA_DIR\"], \"model_{}.pth\".format(model_id))\n",
    "with open(model_path, \"wb\") as f:\n",
    "    torch.save(model.cpu().state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_X = pd.concat([pd.DataFrame(transformed_test_X_len), pd.DataFrame(transformed_test_X)], axis=1)\n",
    "transformed_test_X.columns = [str(i) for i in range(len(transformed_test_X.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transformed_test_X\n",
    "rows = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n",
    "                 dropout_rate, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, ids, length):\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True, \n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "\n",
    "        prediction = self.fc(hidden)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300 # 300\n",
    "hidden_dim = 300 # 300\n",
    "output_dim = len(train_data.unique(\"sentiment\"))\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, \n",
    "             pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 26,772,303 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(77203, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=600, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "                \n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Vectors.__init__() got an unexpected keyword argument 'skiprows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\02-Self_Learning\\01-Data_Science\\11-Sentiment_Analysis\\sentiment_venv\\lib\\site-packages\\torchtext\\vocab\\vectors.py:230\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, language, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_base\u001b[38;5;241m.\u001b[39mformat(language)\n\u001b[0;32m    229\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28msuper\u001b[39m(FastText, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Vectors.__init__() got an unexpected keyword argument 'skiprows'"
     ]
    }
   ],
   "source": [
    "vectors = torchtext.vocab.FastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                      | 0/1245299 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1245299/1245299 [01:46<00:00, 11747.46it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Vector for token b'southfarthing' has 187 dimensions, but previously read vectors have 300 dimensions. All vectors must have the same number of dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m pretrained_embedding \u001b[38;5;241m=\u001b[39m vectors\u001b[38;5;241m.\u001b[39mget_vecs_by_tokens(vocab\u001b[38;5;241m.\u001b[39mget_itos())\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pretrained_embedding\n",
      "File \u001b[1;32m~\\Documents\\02-Self_Learning\\01-Data_Science\\11-Sentiment_Analysis\\sentiment_venv\\lib\\site-packages\\torchtext\\vocab\\vectors.py:230\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, language, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_base\u001b[38;5;241m.\u001b[39mformat(language)\n\u001b[0;32m    229\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28msuper\u001b[39m(FastText, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\02-Self_Learning\\01-Data_Science\\11-Sentiment_Analysis\\sentiment_venv\\lib\\site-packages\\torchtext\\vocab\\vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mzero_ \u001b[38;5;28;01mif\u001b[39;00m unk_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m unk_init\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_vectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\02-Self_Learning\\01-Data_Science\\11-Sentiment_Analysis\\sentiment_venv\\lib\\site-packages\\torchtext\\vocab\\vectors.py:140\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector for token \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, but previously \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread vectors have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions. All vectors must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe same number of dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(word, \u001b[38;5;28mlen\u001b[39m(entries), dim)\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(word, \u001b[38;5;28mbytes\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Vector for token b'southfarthing' has 187 dimensions, but previously read vectors have 300 dimensions. All vectors must have the same number of dimensions."
     ]
    }
   ],
   "source": [
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
    "\n",
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr = 5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch, pad_index):\n",
    "    batch_ids = [i[\"ids\"] for i in batch]\n",
    "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
    "    batch_length = [i[\"length\"] for i in batch]\n",
    "    batch_length = torch.stack(batch_length)\n",
    "    batch_label = [i[\"label\"] for i in batch]\n",
    "    batch_label = torch.stack(batch_label)\n",
    "    batch = {\"ids\": batch_ids,\n",
    "             \"length\": batch_length,\n",
    "             \"label\": batch_label}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "collate = functools.partial(collate, pad_index=pad_index)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               collate_fn=collate, \n",
    "                                               shuffle=True)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
    "        ids = batch['ids'].to(device)\n",
    "        length = batch['length']\n",
    "        label = batch['label'].to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>book_author</th>\n",
       "      <th>reviews</th>\n",
       "      <th>title</th>\n",
       "      <th>reviews_length</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_encode</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36108</th>\n",
       "      <td>11884</td>\n",
       "      <td>John Cheever</td>\n",
       "      <td>cheever writer page book cheever journal entry...</td>\n",
       "      <td>The Journals</td>\n",
       "      <td>780</td>\n",
       "      <td>4.21</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12639</th>\n",
       "      <td>10631</td>\n",
       "      <td>Sam Walton</td>\n",
       "      <td>mnogie dumaiut chto eto sem uolton otkryl form...</td>\n",
       "      <td>Sam Walton: Made In America</td>\n",
       "      <td>700</td>\n",
       "      <td>4.12</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77088</th>\n",
       "      <td>2735</td>\n",
       "      <td>David Guterson</td>\n",
       "      <td>na een mistige nacht wordt vissersboot van car...</td>\n",
       "      <td>Snow Falling On Cedars</td>\n",
       "      <td>1542</td>\n",
       "      <td>3.85</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178903</th>\n",
       "      <td>9296</td>\n",
       "      <td>Isabel Allende</td>\n",
       "      <td>lmwt l wjwd lh y bnty lns ymwtwn ` ndm ytwyhm ...</td>\n",
       "      <td>Eva Luna</td>\n",
       "      <td>142</td>\n",
       "      <td>3.99</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28456</th>\n",
       "      <td>11494</td>\n",
       "      <td>Saul Bellow</td>\n",
       "      <td>humboldt poet revere eventually ridicule charl...</td>\n",
       "      <td>Humboldt's Gift</td>\n",
       "      <td>2961</td>\n",
       "      <td>3.85</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144598</th>\n",
       "      <td>6661</td>\n",
       "      <td>Lawrence C. Ross</td>\n",
       "      <td>member sorority sigma gamma rho sorority book ...</td>\n",
       "      <td>The Divine Nine: The History of African-Americ...</td>\n",
       "      <td>230</td>\n",
       "      <td>4.06</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158837</th>\n",
       "      <td>7562</td>\n",
       "      <td>Chris Ware</td>\n",
       "      <td>guess like better ware sustain narrative like ...</td>\n",
       "      <td>The Acme Novelty Library</td>\n",
       "      <td>677</td>\n",
       "      <td>4.31</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51512</th>\n",
       "      <td>979</td>\n",
       "      <td>Dan Brown</td>\n",
       "      <td>stand dan brown book interesting hold robert l...</td>\n",
       "      <td>Deception Point</td>\n",
       "      <td>934</td>\n",
       "      <td>3.74</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19267</th>\n",
       "      <td>11017</td>\n",
       "      <td>Charlotte BrontÃ«</td>\n",
       "      <td>year saying jane eyre favorite novel time char...</td>\n",
       "      <td>Jane Eyre</td>\n",
       "      <td>404</td>\n",
       "      <td>4.14</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74356</th>\n",
       "      <td>2571</td>\n",
       "      <td>Wole Soyinka</td>\n",
       "      <td>best thing read state anxiety world today mani...</td>\n",
       "      <td>Climate of Fear: The Quest for Dignity in a De...</td>\n",
       "      <td>162</td>\n",
       "      <td>3.62</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38005 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        page_number       book_author   \n",
       "36108         11884      John Cheever  \\\n",
       "12639         10631        Sam Walton   \n",
       "77088          2735    David Guterson   \n",
       "178903         9296    Isabel Allende   \n",
       "28456         11494       Saul Bellow   \n",
       "...             ...               ...   \n",
       "144598         6661  Lawrence C. Ross   \n",
       "158837         7562        Chris Ware   \n",
       "51512           979         Dan Brown   \n",
       "19267         11017  Charlotte BrontÃ«   \n",
       "74356          2571      Wole Soyinka   \n",
       "\n",
       "                                                  reviews   \n",
       "36108   cheever writer page book cheever journal entry...  \\\n",
       "12639   mnogie dumaiut chto eto sem uolton otkryl form...   \n",
       "77088   na een mistige nacht wordt vissersboot van car...   \n",
       "178903  lmwt l wjwd lh y bnty lns ymwtwn ` ndm ytwyhm ...   \n",
       "28456   humboldt poet revere eventually ridicule charl...   \n",
       "...                                                   ...   \n",
       "144598  member sorority sigma gamma rho sorority book ...   \n",
       "158837  guess like better ware sustain narrative like ...   \n",
       "51512   stand dan brown book interesting hold robert l...   \n",
       "19267   year saying jane eyre favorite novel time char...   \n",
       "74356   best thing read state anxiety world today mani...   \n",
       "\n",
       "                                                    title  reviews_length   \n",
       "36108                                        The Journals             780  \\\n",
       "12639                         Sam Walton: Made In America             700   \n",
       "77088                              Snow Falling On Cedars            1542   \n",
       "178903                                           Eva Luna             142   \n",
       "28456                                     Humboldt's Gift            2961   \n",
       "...                                                   ...             ...   \n",
       "144598  The Divine Nine: The History of African-Americ...             230   \n",
       "158837                           The Acme Novelty Library             677   \n",
       "51512                                     Deception Point             934   \n",
       "19267                                           Jane Eyre             404   \n",
       "74356   Climate of Fear: The Quest for Dignity in a De...             162   \n",
       "\n",
       "        rating rating_encode  sentiment  \n",
       "36108     4.21      positive          0  \n",
       "12639     4.12      positive          0  \n",
       "77088     3.85       neutral          1  \n",
       "178903    3.99       neutral          1  \n",
       "28456     3.85       neutral          1  \n",
       "...        ...           ...        ...  \n",
       "144598    4.06      positive          0  \n",
       "158837    4.31      positive          0  \n",
       "51512     3.74       neutral          1  \n",
       "19267     4.14      positive          0  \n",
       "74356     3.62       neutral          1  \n",
       "\n",
       "[38005 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
    "            ids = batch['ids'].to(device)\n",
    "            length = batch['length']\n",
    "            label = batch['label'].to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return epoch_losses, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "    train_losses.extend(train_loss)\n",
    "    train_accs.extend(train_acc)\n",
    "    valid_losses.extend(valid_loss)\n",
    "    valid_accs.extend(valid_acc)\n",
    "    \n",
    "    epoch_train_loss = np.mean(train_loss)\n",
    "    epoch_train_acc = np.mean(train_acc)\n",
    "    epoch_valid_loss = np.mean(valid_loss)\n",
    "    epoch_valid_acc = np.mean(valid_acc)\n",
    "    \n",
    "    if epoch_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = epoch_valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm.pt')\n",
    "    \n",
    "    print(f'epoch: {epoch+1}')\n",
    "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
    "    print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(train_losses, label='train loss')\n",
    "ax.plot(valid_losses, label='valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(train_accs, label='train accuracy')\n",
    "ax.plot(valid_accs, label='valid accuracy')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "\n",
    "epoch_test_loss = np.mean(test_loss)\n",
    "epoch_test_acc = np.mean(test_acc)\n",
    "\n",
    "print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = [vocab[t] for t in tokens]\n",
    "    length = torch.LongTensor([len(ids)])\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor, length).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This app is bad!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
